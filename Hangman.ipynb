{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac43c494-4228-4524-a7a0-70e4bc2f16c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T14:15:06.547738Z",
     "iopub.status.busy": "2023-10-13T14:15:06.547738Z",
     "iopub.status.idle": "2023-10-13T14:15:06.562695Z",
     "shell.execute_reply": "2023-10-13T14:15:06.561694Z",
     "shell.execute_reply.started": "2023-10-13T14:15:06.547738Z"
    }
   },
   "source": [
    "<font size = 6>Improving Hangman Puzzle Strategy, in the Perspective of Reinforcement Learning</font>\n",
    "<font size = 4><div style=\"text-align: right\"> Contributor: Haochen Jiang</div></font>\n",
    "<font size = 4><div style=\"text-align: right\"> June 3, 2023</div></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4738f3-da30-4423-b529-f72e6918a9b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:11:47.950076Z",
     "iopub.status.busy": "2023-10-13T13:11:47.950076Z",
     "iopub.status.idle": "2023-10-13T13:11:52.393290Z",
     "shell.execute_reply": "2023-10-13T13:11:52.392286Z",
     "shell.execute_reply.started": "2023-10-13T13:11:47.950076Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from copy import deepcopy\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8d49b5-d680-4acd-8bcf-4667ed59fa9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:11:52.395774Z",
     "iopub.status.busy": "2023-10-13T13:11:52.395774Z",
     "iopub.status.idle": "2023-10-13T13:11:52.410765Z",
     "shell.execute_reply": "2023-10-13T13:11:52.410765Z",
     "shell.execute_reply.started": "2023-10-13T13:11:52.395774Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "ALPHABET = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 512                                # used in gradient descent\n",
    "LR = 0.00001                                    # learning rate\n",
    "EPSILON = 1                                     # e-greedy policy\n",
    "GAMMA = 1                                       # reward discount coefficient\n",
    "TARGET_REPLACE_ITER = 20000                     # target network update frequency\n",
    "MEMORY_CAPACITY = 10000                          \n",
    "\n",
    "N_STATES = 28                                   # state numebr\n",
    "N_ACTIONS = 26                                  # action number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77763ca4-baf6-4d1c-8700-91496033af9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:11:52.411765Z",
     "iopub.status.busy": "2023-10-13T13:11:52.411765Z",
     "iopub.status.idle": "2023-10-13T13:11:52.426707Z",
     "shell.execute_reply": "2023-10-13T13:11:52.425460Z",
     "shell.execute_reply.started": "2023-10-13T13:11:52.411765Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "def word_format(s):\n",
    "    # pattern = re.compile('.{1}')\n",
    "    # return ' '.join(pattern.findall(s))\n",
    "    return ' '.join(list(s))\n",
    "\n",
    "def find_letters(l, word):\n",
    "    return [i.start() for i in re.finditer(l, word)] \n",
    "\n",
    "def get_multi_hot_encoding(letter_list):\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    result = mlb.fit_transform([tuple(ALPHABET), letter_list])\n",
    "    return result[1]  \n",
    "\n",
    "def get_action_from_index(index_list):\n",
    "    return [ALPHABET[_] for _ in index_list]\n",
    "\n",
    "def get_best_action(action_list, s):\n",
    "    for _ in action_list:\n",
    "        if _ not in s[0]:\n",
    "            return _\n",
    "\n",
    "def fail_reward_gen(reward_list):\n",
    "    # reward_list is stored from 5 to 0\n",
    "    return {_[0]:_[1] for _ in zip(range(5, -1, -1), reward_list)}\n",
    "\n",
    "def get_update_plot(update_nn_list, loss_list):\n",
    "    update_index = (update_nn_list == 0)\n",
    "    \n",
    "    if_update_index = deepcopy(update_nn_list)\n",
    "    \n",
    "    if_update_index[update_index] = 1\n",
    "    if_update_index[~update_index] = 0\n",
    "\n",
    "    y = if_update_index * loss_list\n",
    "    x = if_update_index * (np.arange(len(if_update_index)) + 1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94fe7c74-1b24-4661-a5e7-93dba9f354e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:11:52.429121Z",
     "iopub.status.busy": "2023-10-13T13:11:52.429121Z",
     "iopub.status.idle": "2023-10-13T13:11:52.441509Z",
     "shell.execute_reply": "2023-10-13T13:11:52.441509Z",
     "shell.execute_reply.started": "2023-10-13T13:11:52.429121Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_train_set(seed_num = False):\n",
    "    all_words_path = \"words_250000_train.txt\"\n",
    "    text_file = open(all_words_path, \"r\")\n",
    "    all_words = text_file.read().splitlines()\n",
    "    text_file.close()\n",
    "    if seed_num:\n",
    "        rng = np.random.default_rng(seed = seed_num)\n",
    "        rng.shuffle(all_words)\n",
    "    else:\n",
    "        pass\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c9eb2d-8192-4560-8bc3-f68bcae17088",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:11:52.442509Z",
     "iopub.status.busy": "2023-10-13T13:11:52.442509Z",
     "iopub.status.idle": "2023-10-13T13:11:52.467535Z",
     "shell.execute_reply": "2023-10-13T13:11:52.465544Z",
     "shell.execute_reply.started": "2023-10-13T13:11:52.442509Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class env(object):\n",
    "    def __init__(self, word, succ_reward_each, fail_reward_dict, log = False):\n",
    "        self.log = log\n",
    "        self.status = \"Start\"\n",
    "        self.word = word\n",
    "        \n",
    "        self.reward = 0\n",
    "        self.episode_reward_sum = 0\n",
    "        self.guessed_letters = []\n",
    "        self.tries_remains = 6\n",
    "        self.guessed_word = ''.join([\"_\"] * len(word))\n",
    "        self.completeness = 0\n",
    "        \n",
    "        self.FAIL_GUESS_REWARD = fail_reward_dict\n",
    "        \n",
    "        if self.log:\n",
    "            print(\"Successfully start a new game! # of tries remaining: {0}. 'word': {1}.\".format(self.tries_remains, word_format(word)))\n",
    "    \n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.status == \"Fail\" or self.status == \"Success\":\n",
    "            done = 1\n",
    "        else:\n",
    "            done = 0\n",
    "        return ((self.guessed_letters, self.tries_remains, self.completeness), self.reward, self.episode_reward_sum, self.status, done)\n",
    "    \n",
    "    \n",
    "    def step(self, guess_letter):\n",
    "        if self.status in [\"Fail\", \"Success\"]:\n",
    "            raise Exception(\"This run has already finished !\")  \n",
    "        if self.log:\n",
    "            print(\"Guessing letter: {0}\".format(guess_letter))      \n",
    "        if guess_letter in self.guessed_letters:\n",
    "            raise Exception(\"Cannot guess a letter multiple times !\")\n",
    "        self.guessed_letters.append(guess_letter)\n",
    "        \n",
    "        if guess_letter not in set(self.word):\n",
    "            self.tries_remains -= 1\n",
    "            if self.tries_remains == 0:\n",
    "                self.status = \"Fail\"\n",
    "            else:\n",
    "                self.status = \"Ongoing\"\n",
    "            self.reward = self.FAIL_GUESS_REWARD[self.tries_remains]\n",
    "            if self.log:    \n",
    "                print(\"'status': {}. 'tries_remains': {}. 'word': {}. 'reward': {}. 'completeness': {:.2f}\"\n",
    "                      .format(self.status, self.tries_remains, word_format(self.guessed_word), self.reward, self.completeness))\n",
    "            \n",
    "        else:\n",
    "            self.reward = 0\n",
    "            temp_guessed_word = list(self.guessed_word)\n",
    "            for _ in find_letters(guess_letter, self.word):\n",
    "                temp_guessed_word[_] = guess_letter\n",
    "                self.reward += succ_reward_each\n",
    "            self.guessed_word = \"\".join(temp_guessed_word)\n",
    "            self.completeness = 1 - self.guessed_word.count(\"_\") / len(self.guessed_word)\n",
    "            if self.completeness == 1:\n",
    "                self.status = \"Success\"\n",
    "            else:\n",
    "                self.status = \"Ongoing\"\n",
    "            if self.log:    \n",
    "                print(\"'status': {}. 'tries_remains': {}. 'word': {}. 'reward': {}. 'completeness': {:.2f}\".\n",
    "                      format(self.status, self.tries_remains, word_format(self.guessed_word), self.reward, self.completeness))\n",
    "        \n",
    "        self.episode_reward_sum += GAMMA * self.reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f9ff33-4df5-49e6-8dce-2d46ddbb7727",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T13:11:52.468544Z",
     "iopub.status.busy": "2023-10-13T13:11:52.468544Z",
     "iopub.status.idle": "2023-10-13T13:11:52.493422Z",
     "shell.execute_reply": "2023-10-13T13:11:52.492934Z",
     "shell.execute_reply.started": "2023-10-13T13:11:52.468544Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, 75)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)\n",
    "        #self.fc2 = nn.Linear(100, 100)                                        \n",
    "        #self.fc2.weight.data.normal_(0, 0.1)                                   \n",
    "        self.out = nn.Linear(75, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        # x = F.relu(self.fc2(x))    \n",
    "        actions_value = self.out(x) \n",
    "        return actions_value\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self):  \n",
    "        self.eval_net, self.target_net = Net(), Net()                           # create two neural networks: target network and evaluation network\n",
    "        self.learn_step_counter = 0                                             # for target updating\n",
    "        self.memory_counter = 0                                                 # for memory storing \n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 3))             # initialize memory space (one row stands for a transition)\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)    # Adam optimizer (Input is learning rate and parameters of evaluation network)\n",
    "        self.loss_func = nn.MSELoss()                                 \n",
    "\n",
    "        \n",
    "    def off_policy_action_engine(self, s):\n",
    "        global RANDOM_START\n",
    "        global RANDOM_SEED\n",
    "\n",
    "        if RANDOM_START:\n",
    "            np.random.seed(RANDOM_SEED)\n",
    "            RANDOM_START = False\n",
    "        shuffle_seed = int(np.random.random() * 10e6)\n",
    "        alphabet = set(ALPHABET)\n",
    "        cur_alphabet = list(alphabet - set(s[0]))\n",
    "        rng = np.random.default_rng(seed = shuffle_seed)\n",
    "        rng.shuffle(cur_alphabet)\n",
    "        action = cur_alphabet[0]\n",
    "        return action    \n",
    "          \n",
    "        \n",
    "    def on_policy_action_engine(self, s):\n",
    "        s_tensor = np.hstack((get_multi_hot_encoding(s[0]), [s[1], s[2]]))\n",
    "        s_tensor = torch.unsqueeze(torch.FloatTensor(s_tensor), 0)                # convert s to the form of 32-bit floating point, increase one dimension at dim = 0\n",
    "        if np.random.uniform() < EPSILON:                                         # generate a random number in [0, 1), if it is less than EPSILON, then choose best action\n",
    "            actions_value = self.eval_net.forward(s_tensor)                       \n",
    "            #print(actions_value)\n",
    "            action_sort_index = torch.sort(actions_value, descending=True)[1][0].numpy()\n",
    "            #print(action_sort_index)\n",
    "            #print(torch.sort(actions_value, descending=True)[0][0])\n",
    "            #print()\n",
    "            action_sort_list = get_action_from_index(action_sort_index)     \n",
    "            #print(action_sort_list)\n",
    "            action = get_best_action(action_sort_list, s)\n",
    "        else:\n",
    "            action = self.off_policy_action_engine(s)\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def store_transition(self, s, a, r, done, new_s):\n",
    "        temp_s, temp_new_s = s, new_s\n",
    "        a = ALPHABET.find(a)\n",
    "        s = get_multi_hot_encoding(s[0])\n",
    "        new_s = get_multi_hot_encoding(new_s[0])\n",
    "        \n",
    "        # 在水平方向上拼接数组\n",
    "        transition = np.hstack((s, [temp_s[1], temp_s[2]], [a, r, done], new_s, [temp_new_s[1], temp_new_s[2]]))  \n",
    "        \n",
    "        # overwrite old data if memory space is full\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "        \n",
    "    def learn(self):\n",
    "\n",
    "        update_nn_index = self.learn_step_counter % TARGET_REPLACE_ITER\n",
    "\n",
    "        if update_nn_index == 0:       \n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())     \n",
    "        self.learn_step_counter += 1        \n",
    "\n",
    "        # extract the batch data\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)      \n",
    "        b_memory = self.memory[sample_index, :]       \n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_done = torch.LongTensor(b_memory[:, N_STATES+2:N_STATES+3].astype(int))\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        q_eval = self.eval_net(b_s).gather(dim=1, index=b_a)\n",
    "        next_q_eval = self.target_net(b_s_).max(1)[0].view(BATCH_SIZE, 1).detach()\n",
    "        q_target = b_r + GAMMA * next_q_eval * (1-b_done)\n",
    "        \n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "        loss_num = loss.detach().numpy()\n",
    "        writer.add_scalar('Loss', loss_num, self.learn_step_counter)\n",
    "        \n",
    "        self.optimizer.zero_grad()   \n",
    "        loss.backward()       \n",
    "        self.optimizer.step()                  \n",
    "        \n",
    "        return update_nn_index, loss_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c22a4c-bc15-4db6-9366-4291bad4d276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dqn = DQN()                                                       \n",
    "\n",
    "all_words = generate_train_set(353)\n",
    "word_list = all_words * 10\n",
    "\n",
    "fail_reward_dict = fail_reward_gen([-5, -5, -5, -5, -5, -5])\n",
    "succ_reward_each = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir = \"D:/Software_Workspace/Tensorboard_Workspace/\")\n",
    "\n",
    "RANDOM_START = True\n",
    "RANDOM_SEED = 343\n",
    "\n",
    "PLOT_WARM_UP = False\n",
    "PLOT_INTERVAL = 100\n",
    "PLOT_ACCURATE_RANGE = 200\n",
    "\n",
    "# The following variables are for plotting\n",
    "r_list = np.array([])\n",
    "c_list = np.array([])\n",
    "loss_list = np.array([])\n",
    "update_nn_list = np.array([])\n",
    "\n",
    "writer.add_scalar('Accuracy', 0, 1)\n",
    "accurate_list = np.array([0])\n",
    "accurate_index = np.array([1])\n",
    "\n",
    "%matplotlib qt5\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "\n",
    "for n_iter in range(1, 100001):\n",
    "    print('<<<<<<<<<< Episode - %s' % n_iter)\n",
    "    cur_env = env(word_list[n_iter - 1], succ_reward_each, fail_reward_dict, False)\n",
    "    s = deepcopy(cur_env.get_state()[0])\n",
    "\n",
    "    while True:                        # begin an episode (one loop for an step)\n",
    "        #print(\"------------------------------------------\")\n",
    "        #print(\"States\")\n",
    "        #print(s)\n",
    "        \n",
    "        #a = dqn.off_policy_action_engine(s) \n",
    "        a = dqn.on_policy_action_engine(s)\n",
    "        cur_env.step(a)    \n",
    "        \n",
    "        # Get interaction from last step\n",
    "        new_s, r, reward_sum, status, done = deepcopy(cur_env.get_state())\n",
    "\n",
    "\n",
    "        #print((s, a, r, new_s))\n",
    "        dqn.store_transition(s, a, r, done, new_s)                 # store a transiton sample\n",
    "                        \n",
    "        #print(\"------------------------------------------\")\n",
    "\n",
    "        s = new_s\n",
    "        #print(s)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:       \n",
    "            \n",
    "            if not PLOT_WARM_UP:\n",
    "                print(\"\\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Warming Up End ! <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\\n\")\n",
    "                print(\"At Episode : %d .\" % n_iter)\n",
    "                print(\"\\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Warming Up End ! <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\\n\")\n",
    "                PLOT_WARM_UP = n_iter\n",
    "                \n",
    "            learn_results = dqn.learn()\n",
    "            update_nn_list = np.append(update_nn_list, learn_results[0])\n",
    "            loss_list = np.append(loss_list, learn_results[1])\n",
    "            \n",
    "                \n",
    "        if status == \"Fail\" or status == \"Success\":   # Finish\n",
    "            r_list = np.append(r_list, reward_sum)\n",
    "            writer.add_scalar('Reward Sum', r_list[-1], n_iter)\n",
    "            c_list = np.append(c_list, s[2])\n",
    "            writer.add_scalar('Completeness', c_list[-1], n_iter)\n",
    "            \n",
    "\n",
    "            \n",
    "            if not PLOT_WARM_UP:\n",
    "                if n_iter % PLOT_INTERVAL == 0:\n",
    "                    plt.clf()          # clear previous picture\n",
    "                    \n",
    "                    x_index = np.arange(len(r_list)) + 1\n",
    "                    plt.subplot(2, 2, 1)\n",
    "                    plt.plot(x_index, r_list)\n",
    "                    plt.title(\"Reward Sum\")\n",
    "\n",
    "                    plt.subplot(2, 2, 2)\n",
    "                    plt.plot(np.arange(len(loss_list)) + 1, loss_list)\n",
    "                    plt.title(\"Loss\")\n",
    "                    \n",
    "                    plt.subplot(2, 2, 3)\n",
    "                    plt.plot(x_index, c_list)\n",
    "                    plt.title(\"Completeness\")\n",
    "\n",
    "                    plt.pause(0.001)   # pause for a while, or it will be very lagged\n",
    "                    plt.ioff()         # close real-time window\n",
    "                \n",
    "            else:\n",
    "                if n_iter % PLOT_INTERVAL == 0:\n",
    "                    accurate_list = np.append(accurate_list, (c_list[-PLOT_ACCURATE_RANGE:] == 1).sum() / PLOT_ACCURATE_RANGE)\n",
    "                    accurate_index = np.append(accurate_index, n_iter)\n",
    "                    writer.add_scalar('Accuracy', accurate_list[-1], n_iter)\n",
    "                    \n",
    "                    plt.clf()\n",
    "                    \n",
    "                    x_index = np.arange(len(r_list)) + 1\n",
    "                    plt.subplot(2, 2, 1)\n",
    "                    plt.plot(x_index, r_list)\n",
    "                    plt.plot([PLOT_WARM_UP] * 100, np.linspace(min(r_list) - 1, max(r_list) + 1, 100), c = \"r\", linestyle = \"--\", zorder =2)\n",
    "                    plt.title(\"Reward Sum\")\n",
    "                    \n",
    "                    x, y = get_update_plot(update_nn_list, loss_list)\n",
    "                    plt.subplot(2, 2, 2)\n",
    "                    plt.plot(np.arange(len(loss_list)) + 1, loss_list)\n",
    "                    plt.plot(x, y, lw = 0, marker = \"o\", ms = 2.5, c = \"r\", zorder = 2)\n",
    "                    plt.title(\"Loss\")\n",
    "                    \n",
    "                    plt.subplot(2, 2, 3)\n",
    "                    plt.plot(x_index, c_list)\n",
    "                    plt.plot([PLOT_WARM_UP] * 100, np.linspace(-0.03, 1.03, 100), c = \"r\", linestyle = \"--\", zorder =2)\n",
    "                    plt.title(\"Completeness\")\n",
    "                    \n",
    "                    plt.subplot(2, 2, 4)\n",
    "                    plt.plot(accurate_index, accurate_list)\n",
    "                    plt.title(\"Accuracy\")\n",
    "                    \n",
    "                    plt.pause(0.001)\n",
    "                    plt.ioff()\n",
    "            \n",
    "            \n",
    "            # Use round() to return a formatted float\n",
    "            print('>>>>>>>>>> Episode - %s     Reward_sum: %s' % (n_iter, round(reward_sum, 2)))\n",
    "            print(\"==================================================================================================\")\n",
    "            # Do not forget to use break to go out of While loop\n",
    "            break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089867e-af78-4cef-b8b1-ea0f589434e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ee4ae-948f-49c0-8b7e-5aabcf844c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e0c934-e03e-4187-baaf-7227d8b14622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tensorboard real-time update is extremely slow.\n",
    "# So it is better to execute the following code after we have finished training.\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir \"D:/Software_Workspace/Tensorboard_Workspace/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8c66b-3a35-4989-8003-f357e8157b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
